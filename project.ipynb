{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba39ef4-c37a-4ba3-903f-98c2851260bc",
   "metadata": {},
   "source": [
    "# Read in Word Doc\n",
    "\n",
    "Grab the text from a word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7889e4-2b9c-4be6-bdc1-6da4ac8ae66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Callable\n",
    "from functools import reduce, partial\n",
    "\n",
    "from docx import Document\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7134ab-2c82-4bff-a06e-2eac3e7504b4",
   "metadata": {},
   "source": [
    "Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b74e02-6cb8-4825-b887-009a39d6c7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline(*functions: Callable) -> Callable:\n",
    "    '''\n",
    "    create a callable pipeline of functions; \n",
    "    functions f, g,...n become a single callable of  n(...(g(f(x))))\n",
    "    '''\n",
    "    return reduce(lambda f, g: lambda x: g(f(x)), functions)\n",
    "\n",
    "def flatten(x: list) -> list:\n",
    "    return sum(map(flatten, x), []) if isinstance(x, list) else [x]\n",
    "\n",
    "\n",
    "def clean(text: str) -> list[str]:\n",
    "    # pipeline funcs\n",
    "    tokenize = lambda x: nltk.word_tokenize(x)\n",
    "    \n",
    "    # regexes\n",
    "    url_re = lambda s: s if s and re.match(('(https?:\\/\\/)?([\\w\\-])+\\.{1}'\n",
    "                                         '([a-zA-Z]{2,63})([\\/\\w-]*)*\\/?\\??'\n",
    "                                         '([^#\\n\\r]*)?#?([^\\n\\r]*)'), s) is None else \"URL\"\n",
    "    uname_re = lambda s: s if s and re.match(r'^@\\S+', s) is None else \"SCREEN_NAME\"\n",
    "    hashtag_re = lambda s: s if s and re.match(r'^#\\S+', s) is None else \"HASHTAG\"\n",
    "    \n",
    "    # filters/maps\n",
    "    url = lambda x: map(url_re, x)\n",
    "    uname = lambda x: map(uname_re, x)\n",
    "    hashtag = lambda x: map(hashtag_re, x)\n",
    "    lower = lambda x: map(lambda s: s.lower(), x)\n",
    "    len_4 = lambda x: filter(lambda wd: len(wd) > 4, x)\n",
    "    #frontslash = lambda x: map(lambda s: s.split('/'), x) # this is dumb\n",
    "    \n",
    "    f = pipeline(tokenize, lower, url, uname, hashtag, len_4)\n",
    "    \n",
    "    return list(f(text))\n",
    "    \n",
    "def lemmatize(tokens: list[str]) -> list[str]:\n",
    "    # whole-token related\n",
    "    en_stop = lambda x: filter(lambda wd: wd not in stopwords.words('english'), x)\n",
    "    lemmywinks = lambda x: map(lambda wd: wn.morphy(wd) or wd, x)\n",
    "        \n",
    "    f = pipeline(en_stop, lemmywinks)\n",
    "    \n",
    "    return list(f(tokens))\n",
    "\n",
    "def get_document_words(path: str) -> list[str]:\n",
    "    'get the words from a word document and return as a list of tokens'\n",
    "       \n",
    "    doc = Document(path)\n",
    "    \n",
    "    lines = [list(clean(para.text)) for para in doc.paragraphs]\n",
    "    \n",
    "    return list(lemmatize(flatten(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4dee6-df91-4975-96f7-809b816fd795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_document_words(\"Luke Chambers FT Resume RES-2020-00386.docx\")[5:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61ee9d-3386-4be0-9193-d8fd1b7f13e1",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "\n",
    "the data set is labeled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ecb01-6d18-4eb6-ad40-8e4c08e56007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('UpdatedResumeDataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dabd4f-8549-4573-9c01-db5f98ecadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:500:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x=df['Category'].value_counts(),\n",
    "            y=df['Category'].value_counts().index,\n",
    "            palette='icefire_r');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5d8ea-ec4a-4f4b-b0ca-06e8ed7d3861",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60323ac2-a004-41c0-b4ee-6315daf933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_text(text: str) -> list[str]:\n",
    "    tokens = clean(text)\n",
    "    return lemmatize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c3755-ec4c-4e67-815b-e18dd1d1896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook\n",
    "\n",
    "text_data = []\n",
    "for text in tqdm.notebook.tqdm_notebook(df['Resume']):\n",
    "    text_data.append(prep_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2aaa3c-88d6-4dc8-a306-18a2110f69e6",
   "metadata": {},
   "source": [
    "Create dictionary and corpus from the resume text data -- resumable from here for time's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557a81f-c4a2-4c6d-8c7f-0b1ed308d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0177ad5-9c19-4320-a7f9-2f6546abb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c600120-796a-4002-a5c0-e3343390836d",
   "metadata": {},
   "source": [
    "Convenience cell for reload on restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66a3cb-3eb7-4e70-a9ef-86d8de444752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os, pickle\n",
    "\n",
    "#if not globals().get('dictionary'):\n",
    "#    if os.path.exists('corpus.pkl') and os.path.exists('dictionary.gensim'):\n",
    "#        corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "#        dictionary = corpora.Dictionary().load(\"dictionary.gensim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6485c-d7a9-4c24-8362-c34762f68802",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5b3af-17e7-458d-87df-a2239cc467d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = len(df['Category'].unique())\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus,\n",
    "    num_topics=15,\n",
    "    id2word=dictionary,\n",
    "    passes=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033be39-4962-4b6a-a58a-f2ada7e28a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_words=3)\n",
    "for topic in sorted(topics, key=lambda k: k[0]):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7458b77-0981-4bca-b7e6-7b8d118d41b2",
   "metadata": {},
   "source": [
    "What topic is *my* resume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b471a7-b312-4b74-8709-fb6d645f25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = get_document_words('Luke Chambers FT Resume RES-2020-00386.docx')\n",
    "\n",
    "new_doc_bow =dictionary.doc2bow(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b3e1f-598b-4589-bc4f-6ffa080750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lda_model.get_document_topics(new_doc_bow), key=lambda k: k[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55c19b-4af9-47bf-a6ba-ed71c86dcd87",
   "metadata": {},
   "source": [
    "# Mentioning ATB\n",
    "\n",
    "just for funzies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aec39-112b-455d-848a-62aaf4faa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ATB.atb import ATB\n",
    "\n",
    "atb = ATB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673db26-4583-4bc7-a539-fec46bc3f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def make_vocabulary() -> set:\n",
    "    '''\n",
    "    Create a term vocabulary for ingest and comparisons\n",
    "    '''\n",
    "    words = {w for word in open('words.txt', 'r').readlines()\n",
    "            if (w:=re.sub('[\\W]+', '', word.strip().lower()))}\n",
    "\n",
    "    packs = {d.get('project').lower()\n",
    "             for d in json.loads(open('pypi.json').read())['rows']}\n",
    "    packs = packs.union({t.strip().lower()\n",
    "                         for t in json.loads(open('technicalterms.json').read())})\n",
    "    for pack in packs:\n",
    "        words.add(pack)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec985-b0c2-416b-a46f-87bb4435aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = make_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b286a52-52ab-4aa8-b477-286946d1b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook\n",
    "for word in tqdm.notebook.tqdm_notebook(words):\n",
    "    atb.insert(word, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b2eba-e494-40b4-ad56-767883595808",
   "metadata": {},
   "outputs": [],
   "source": [
    "atb.dijkstra_segment(\"squashedtogetherwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33e116-9bdc-4e8d-a28b-b2365876fda3",
   "metadata": {},
   "source": [
    "# Make a Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be1a13-4c59-4ea2-ae95-4ea7741cc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0cb6a7-4ace-4052-8007-9c5442b7d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "\n",
    "df['cat'] = label.fit_transform(df['Category'])\n",
    "df['clean'] = [\" \".join(thing) for thing in text_data]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b49ea5-6e99-43b0-8d34-ba6fa198fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['clean'].values\n",
    "target = df['cat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fddbf42-ae41-4918-bac2-94edc2910a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english',\n",
    "    max_features=1500,\n",
    ")\n",
    "word_vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdfe9d0-e5a4-43bb-ae6e-1cd0d9a659b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = word_vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a15831-b3f1-47df-a2dd-7bfc1e4848c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, target, random_state=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2456560-1a2e-4e45-b95e-26238c8ed2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(KNeighborsClassifier())\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241d689-7044-4c22-a1f7-ee2c977c39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba8e34-7754-4801-97b5-3ea68c7bda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'  Training Accuracy: {model.score(x_train, y_train):.2%}')\n",
    "print(f'Validation Accuracy: {model.score(x_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b5753-b434-4e20-9bd9-f7d8ce11d1d0",
   "metadata": {},
   "source": [
    "# ...And a GUI because why not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81deb9a6-67bf-4af0-b005-4c34097dd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as wid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3ce75-b1aa-41c3-af29-f15548a4481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "paste_area = wid.Textarea(layout={'height': '300px', 'width':'75%'})\n",
    "butt = wid.Button(description = 'Classify')\n",
    "out = wid.Output(layout={'height':'100px',\n",
    "                         'width':'75%',\n",
    "                         'margin': '50px auto',\n",
    "                         'border':'1px solid darkgrey'})\n",
    "\n",
    "@out.capture()\n",
    "def click(context) -> None:\n",
    "    text = ' '.join(prep_text(paste_area.value))\n",
    "    text_features = word_vectorizer.transform([text])\n",
    "    result = model.predict(text_features)\n",
    "    print(\"Resume classification:\", end=' ')\n",
    "    print(label.inverse_transform(result)[0])\n",
    "        \n",
    "\n",
    "butt.on_click(click)\n",
    "        \n",
    "box = wid.VBox(children=[\n",
    "    wid.Label('Paste Resume Text into this area and hit the button!'),\n",
    "    paste_area,\n",
    "    butt,\n",
    "    out\n",
    "], layout={'display': 'flex',\n",
    "           'flex_flow':'column',\n",
    "           'align_items':'center'})\n",
    "\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8c2ed-0386-41ab-a56f-43a30fe7aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \" \".join(get_document_words(\"Luke Chambers FT Resume RES-2020-00386.docx\"))\n",
    "doc_features = word_vectorizer.transform([doc])\n",
    "result = model.predict(doc_features)\n",
    "\n",
    "label.inverse_transform(result)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
